{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "genuine-protocol",
   "metadata": {},
   "source": [
    "# 텍스트 요약\n",
    "- 긴 길이의 문서(Document) 원문을 핵심 주제만으로 구성된 짧은 요약(Summary) 문장들로 변환하는 것.\n",
    "-  상대적으로 큰 텍스트인 뉴스 기사로 작은 텍스트인 뉴스 제목을 만들어내는 것이 텍스트 요약의 대표적인 예.\n",
    "- 요약 전후에 정보 손실 발생이 최소화되어야 함.\n",
    "- 텍스트의 길이가 크게 줄어들었지만, 요약문은 문서 원문이 담고 있는 정보를 최대한 보존하고 있어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-refrigerator",
   "metadata": {},
   "source": [
    "## 추출적 요약\n",
    "- 단어 그대로 원문에서 문장들을 추출해서 요약하는 방식.\n",
    "- 0개의 문장으로 구성된 텍스트가 있다면, 그중 핵심적인 문장 3개를 꺼내와서 3개의 문장으로 구성된 요약문을 만드는 식.\n",
    "- 꺼내온 3개의 문장이 원문에서 중요한 문장일 수는 있어도, 3개의 문장의 연결이 자연스럽지 않을 수는 있음.(문장들 간 호응이 자연스럽지 않을 수 있다는 의미.)\n",
    "- 딥 러닝보다는 주로 전통적인 머신 러닝 방식에 속하는 텍스트 랭크(TextRank)와 같은 알고리즘을 사용해서 이 방법을 사용함.\n",
    "- 가장 대표적인 것이 네이버 뉴스 서비스에 있는 요약봇 기능.\n",
    "- TextRank 알고리즘을 통해 해당 기사를 가장 잘 대표하는 단어들로 이루어진 핵심문장을 아주 효과적으로 찾아냄.\n",
    "- 원문을 구성하는 문장 중 어느 것이 요약문에 들어갈 핵심문장인지를 판별한다는 점에서 문장 분류(Text Classification) 문제로 볼 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-medicaid",
   "metadata": {},
   "source": [
    "## 추상적 요약\n",
    "-  원문으로부터 내용이 요약된 새로운 문장을 생성해냄.\n",
    "- 새로운 문장이라는 것은 결과로 나온 문장이 원문에 원래 없던 문장일 수도 있다는 것을 의미.\n",
    "- 자연어 처리 분야 중 자연어 생성(Natural Language Generation, NLG)의 영역임.\n",
    "- 구글에서 텍스트 요약을 위해 시도했던 접근법 중에, 텍스트마이닝 분야의 '역문서빈도(IDF)같은' 지표를 활용해 문서 안에서 중요해 보이는 부분을 추출하고 그걸 요약문에 담는 방식을 썼음.\n",
    "- 구글의 방식은 어색하거나 문법적으로 이상한 결과물을 만드는 문제가 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-triple",
   "metadata": {},
   "source": [
    "# seq2seq 모델로 추출적 요약 방식 텍스트 요약기 만들기\n",
    "\n",
    "\n",
    "   \n",
    "1. seq2seq 개요\n",
    "- seq2seq은 두 개의 RNN 아키텍처를 사용하여 입력 시퀀스로부터 출력 시퀀스를 생성해내는 자연어 생성 모델임.\n",
    "- 원문을 첫 번째 RNN인 인코더로 입력하면, 인코더는 이를 하나의 고정된 벡터로 변환.\n",
    "- 이 벡터를 문맥 정보를 가지고 있는 벡터라고 하여 컨텍스트 벡터(context vector)라고 함. \n",
    "- 두 번째 RNN인 디코더는 이 컨텍스트 벡터를 전달받아 한 단어씩 생성해내서 요약 문장을 완성함.\n",
    "   \n",
    "   \n",
    "2. LSTM과 컨텍스트 벡터\n",
    "- seq2seq를 구현할 때, 인코더/디코더로 바닐라 RNN이 아니라 LSTM을 사용.\n",
    "- LSTM이 바닐라 RNN과 다른 점은 다음 time step의 셀에 hidden state뿐만 아니라, cell state도 함께 전달함.\n",
    "- 인코더가 디코더에 전달하는 컨텍스트 벡터 또한 hidden state h와 cell state c 두 개의 값 모두 존재해야 한다는 뜻.\n",
    "- 기본적인 seq2seq 모델에서 컨텍스트 벡터는 디코더의 현재 스텝 위치에 무관하게 한번 계산되면 고정값을 가짐.\n",
    "   \n",
    "   \n",
    "3. 시작 토큰과 종료 토큰\n",
    "- seq2seq 구조에서 디코더는 시작 토큰 SOS가 입력되면, 각 시점마다 단어를 생성하고 이 과정을 종료 토큰 EOS를 예측하는 순간까지 멈추지 않음.\n",
    "- 데이터의 예측 대상 시퀀스의 앞, 뒤에는 시작 토큰과 종료 토큰을 넣어주는 전처리를 통해 어디서 멈춰야 하는지 알려줄 필요가 있음.   \n",
    "   \n",
    "   \n",
    "4. 어텐션 메커니즘을 통한 새로운 컨텍스트 벡터 사용\n",
    "- 기존에 배운 seq2seq를 수정하고, 새로운 모듈을 붙여 모델의 성능을 높일 것.\n",
    "- 기존의 seq2seq는 인코더의 마지막 time step의 hidden state를 컨텍스트 벡터로 사용.\n",
    "- RNN 계열의 인공 신경망(바닐라 RNN, LSTM, GRU)의 한계로 인해 이 컨텍스트 정보에는 이미 입력 시퀀스의 많은 정보가 손실이 된 상태가 됨.\n",
    "- 어텐션 메커니즘(Attention Mechanism) 은 이와 달리, 인코더의 모든 step의 hidden state의 정보가 컨텍스트 벡터에 전부 반영되도록 하는 것.\n",
    "- 인코더의 모든 hidden state가 동일한 비중으로 반영되는 것이 아니라, 디코더의 현재 time step의 예측에 인코더의 각 step이 얼마나 영향을 미치는지에 따른 가중합으로 계산되는 방식.\n",
    "- 컨텍스트 벡터를 구성하기 위한 인코더 hidden state의 가중치 값은 디코더의 현재 스텝이 어디냐에 따라 계속 달라진다는 점을 주의해야 함.\n",
    "- 디코더의 현재 문장 생성 부위가 주어부인지 술어부인지 목적어인지 등에 따라 인코더가 입력 데이터를 해석한 컨텍스트 벡터가 다른 값이 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-month",
   "metadata": {},
   "source": [
    "# 아마존 리뷰 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "jewish-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Error with downloaded zip file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "#NLTK의 불용어(stopwords)를 사용할 것.\n",
    "#NTLK와 데이터셋을 설치, 다운\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-bouquet",
   "metadata": {},
   "source": [
    "- NLTK는 Natural Language Toolkit의 축약어로 영어 기호, 통계, 자연어 처리를 위한 라이브러.\n",
    "- NLTK에는 I, my, me, over, 조사, 접미사와 같이 문장에는 자주 등장하지만, 의미를 분석하고 요약하는 데는 거의 의미가 없는 100여개의 불용어가 미리 정리되어 있음.\n",
    "- 이를 이용해 다운로드한 리뷰 파일에서 불용어를 제거하는 작업을 진행할 예정.\n",
    "- 링크에서 다운로드 받은 데이터(Reviews.csv)는 총 568,454개의 샘플을 갖고 있음.\n",
    "- 모든 샘플 사용하지 않고 10만개의 샘플만 사용할 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "concrete-firewall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    }
   ],
   "source": [
    "#10만개의 샘플만 사용\n",
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/data/Reviews.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "micro-richards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approved-wisdom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73374</th>\n",
       "      <td>Taste is good, but I did need to add some sesa...</td>\n",
       "      <td>Very nice for stir fry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61526</th>\n",
       "      <td>I have tried all of the Looza drinks but the B...</td>\n",
       "      <td>Amazing Banana Drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74200</th>\n",
       "      <td>This is very grainy and chalky like most other...</td>\n",
       "      <td>Manitoba Dark Chocolate Hemp Powder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66707</th>\n",
       "      <td>this tea is the best around. it has full rich ...</td>\n",
       "      <td>ahmad tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38515</th>\n",
       "      <td>I read a &lt;a href=\"http://www.amazon.com/gp/pro...</td>\n",
       "      <td>A guilty pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98381</th>\n",
       "      <td>I love Larabars, and have about 1 a day during...</td>\n",
       "      <td>Moldy!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18646</th>\n",
       "      <td>I read the reviews and was dubious but my dog ...</td>\n",
       "      <td>Amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10606</th>\n",
       "      <td>My papillon has been eating this for a while a...</td>\n",
       "      <td>Loves it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72336</th>\n",
       "      <td>When I was a kid, Post used to make a cereal c...</td>\n",
       "      <td>Fantastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17309</th>\n",
       "      <td>I've served these several times and despite my...</td>\n",
       "      <td>Real Brownies!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27246</th>\n",
       "      <td>We use many K cups.This fits in well with the ...</td>\n",
       "      <td>Great K cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26500</th>\n",
       "      <td>I took advantage of the 50% off special last s...</td>\n",
       "      <td>Surprisingly flexible additive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56651</th>\n",
       "      <td>Used to eat the Spicy Thai flavor all the time...</td>\n",
       "      <td>Kettle has sold out, the chips are horrible now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49206</th>\n",
       "      <td>I have bought these multiple times and let man...</td>\n",
       "      <td>Excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68861</th>\n",
       "      <td>delivered in a timely manner and as described....</td>\n",
       "      <td>as good as my country store at much less the p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "73374  Taste is good, but I did need to add some sesa...   \n",
       "61526  I have tried all of the Looza drinks but the B...   \n",
       "74200  This is very grainy and chalky like most other...   \n",
       "66707  this tea is the best around. it has full rich ...   \n",
       "38515  I read a <a href=\"http://www.amazon.com/gp/pro...   \n",
       "98381  I love Larabars, and have about 1 a day during...   \n",
       "18646  I read the reviews and was dubious but my dog ...   \n",
       "10606  My papillon has been eating this for a while a...   \n",
       "72336  When I was a kid, Post used to make a cereal c...   \n",
       "17309  I've served these several times and despite my...   \n",
       "27246  We use many K cups.This fits in well with the ...   \n",
       "26500  I took advantage of the 50% off special last s...   \n",
       "56651  Used to eat the Spicy Thai flavor all the time...   \n",
       "49206  I have bought these multiple times and let man...   \n",
       "68861  delivered in a timely manner and as described....   \n",
       "\n",
       "                                                 Summary  \n",
       "73374                            Very nice for stir fry.  \n",
       "61526                               Amazing Banana Drink  \n",
       "74200                Manitoba Dark Chocolate Hemp Powder  \n",
       "66707                                          ahmad tea  \n",
       "38515                                  A guilty pleasure  \n",
       "98381                                           Moldy!!!  \n",
       "18646                                            Amazing  \n",
       "10606                                           Loves it  \n",
       "72336                                          Fantastic  \n",
       "17309                                     Real Brownies!  \n",
       "27246                                        Great K cup  \n",
       "26500                     Surprisingly flexible additive  \n",
       "56651    Kettle has sold out, the chips are horrible now  \n",
       "49206                                          Excellent  \n",
       "68861  as good as my country store at much less the p...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#전체 데이터 중 summary열과 text열만 훈련에서 사용 할 것.\n",
    "#해당 열만 별도로 저장하면 됨.\n",
    "\n",
    "data = data[['Text','Summary']]\n",
    "data.head()\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-rebel",
   "metadata": {},
   "source": [
    "- Text 열의 내용을 요약한 것이 Summary 열.\n",
    "- 인공 신경망을 통해 Text 시퀀스를 입력받으면 Summary 시퀀스를 예측하도록 훈련할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-bronze",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-sampling",
   "metadata": {},
   "source": [
    "### - 중복, NULL 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collaborative-scheduling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "timely-somewhere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "#inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "awful-jamaica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aerial-elements",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-surrey",
   "metadata": {},
   "source": [
    "### - 텍스트 정규화\n",
    "- 같은 의미인데도 다른 표현으로 쓰여 마치 다른 단어들처럼 간주되는 경우는 텍스트 정규화를 통해 학습 전 미리 같은 표현으로 통일시켜야 함.\n",
    "- 텍스트 정규화를 위한 사전(dictionary)을 아래와 같이 구성할 것.\n",
    "- 텍스트 정규화 사전 출처 : [사전 출처](https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chinese-lithuania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-footwear",
   "metadata": {},
   "source": [
    "### - 데이터 전처리 함수 생성\n",
    "- NLTK에서 제공하는 불용어 리스트를 참조해, 샘플에서 불용어를 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-mixer",
   "metadata": {},
   "source": [
    "- 불용어 제거 및 모든 영어 문자는 소문자로 만들고, 섞여있는 html 태그를 제거하고, 정규 표현식을 통해 각종 특수문자를 제거할 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "primary-message",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dying-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() #텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    #상대적으로 문장 길이가 짧은 Summary 전처리할 때는 호출하지 않음.\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "described-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything bought great infact ordered twice third ordered wasfor mother father\n",
      "great way to start the day\n"
     ]
    }
   ],
   "source": [
    "#전처리 결과 확인\n",
    "\n",
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(preprocess_sentence(temp_text))\n",
    "\n",
    "#Text의 경우에는 불용어를 제거하고, Summary의 경우에는 불용어를 제거하지 않음.\n",
    "print(preprocess_sentence(temp_summary, False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-prior",
   "metadata": {},
   "source": [
    "### - 훈련 데이터 전체에 대해 전처리 수행\n",
    "- 위와 같이 Text의 경우에는 불용어를 제거하고, Summary의 경우에는 불용어를 제거하지 않을 것이므로 따로 호출해서 진행해야 함. \n",
    "- Text를 먼저 전처리하고, 결과를 확인하기 위해서 상위 5개의 줄을 출력해볼 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "virgin-microphone",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-48b7854b3fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 전처리 후 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b7ac5f0b173f>\u001b[0m in \u001b[0;36mpreprocess_sentence\u001b[0;34m(sentence, remove_stopwords)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 불용어 제거 (Text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 불용어 미제거 (Summary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#상대적으로 문장 길이가 짧은 Summary 전처리할 때는 호출하지 않음.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b7ac5f0b173f>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 불용어 제거 (Text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 불용어 미제거 (Summary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#상대적으로 문장 길이가 짧은 Summary 전처리할 때는 호출하지 않음.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     21\u001b[0m         return [\n\u001b[1;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         ]\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### 싱글 프로세스\n",
    "### 굉장히 오래 걸림\n",
    "\n",
    "clean_text = []\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(clean_text[:5])\n",
    "\n",
    "\n",
    "clean_summary = []\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(clean_summary[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "shared-scholar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448.8667562007904  seconds\n",
      "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better'\n",
      " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo'\n",
      " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch'\n",
      " ...\n",
      " 'favorite brand korean ramen spicy used eating spicy food make sure use spice pack add egg soup makes great snack'\n",
      " 'like noodles although say spicy somewhat understatement one else family tolerates spicy well seeing looking forward extra little something palate disappointed completely honest usually drain liquid almost much'\n",
      " 'love noodle twice week amazing thing feel well cold hot bowl noodle cure upset stomach headache running nose may work definitely try']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.518531799316406  seconds\n",
      "['good quality dog food' 'not as advertised' 'delight says it all' ...\n",
      " 'great ramen' 'spicy'\n",
      " 'this spicy noodle cures my cold upset stomach and headache every time']\n"
     ]
    }
   ],
   "source": [
    "### 멀티 프로세싱\n",
    "\n",
    "import multiprocessing as mp   # 멀티 프로세싱으로 전처리 속도를 획기적으로 줄여봅시다\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial  # map을 할 때 함수에 여러 인자를 넣어줄 수 있도록 합니다\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# num_cores 만큼 쪼개진 데이터를 전처리하여 반환합니다\n",
    "def appendTexts(sentences, remove_stopwords):\n",
    "  texts = []\n",
    "  for s in sentences:\n",
    "    texts += preprocess_sentence(s, remove_stopwords),\n",
    "  return texts\n",
    "\n",
    "def preprocess_data(data, remove_stopwords=True):\n",
    "  start_time = time.time()\n",
    "  num_cores = mp.cpu_count()  # 컴퓨터의 코어 수를 구합니다\n",
    "\n",
    "  text_data_split = np.array_split(data, num_cores)  # 코어 수만큼 데이터를 배분하여 병렬적으로 처리할 수 있게 합니다\n",
    "  pool = Pool(num_cores)\n",
    "\n",
    "  processed_data = np.concatenate(pool.map(partial(appendTexts, remove_stopwords=remove_stopwords), text_data_split))  # 각자 작업한 데이터를 하나로 합쳐줍니다\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  print(time.time() - start_time, \" seconds\")\n",
    "  return processed_data\n",
    "\n",
    "clean_text = preprocess_data(data['Text'])  # 클라우드 기준으로 3~4분 정도 소요 됩니다\n",
    "print(clean_text)\n",
    "\n",
    "#Summary에 대해서 전처리 함수를 호출해 줄 때는, 불용어 제거를 수행하지 않는다는 의미에서 두 번째 인자로 False 넣음.\n",
    "clean_summary = preprocess_data(data['Summary'], remove_stopwords=False) # 클라우드 기준 1분정도 소요됩니다.\n",
    "print(clean_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "studied-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 후 빈 샘플이 생겼는지 확인\n",
    "#전처리 과정에서 문장의 모든 단어가 사라지는 경우가 있음.\n",
    "#빈 샘플이 있으면 모두 Null로 대체\n",
    "\n",
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "infrared-building",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-structure",
   "metadata": {},
   "source": [
    "- Summary 열에서 70개의 Null 값이 생겼음.\n",
    "- 원래는 단어가 있었는데, 정제 과정에서 모든 단어가 제거되어 빈 샘플이 70개나 생겼으니 이 샘플들은 모두 제거함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lasting-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-evening",
   "metadata": {},
   "source": [
    "### - 데이터셋 분리(훈련, 테스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "southeast-watson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####샘플의 최대 길이 정하기\n",
    "\n",
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-meditation",
   "metadata": {},
   "source": [
    "- Text의 경우 최소 길이가 2, 최대 길이가 1,235, 평균 길이는 38로 그래프로 봤을 때는 대체적으로는 100 내외의 길이를 가진다는 것을 알 수 있음.\n",
    "- Summary의 경우 최소 길이가 1, 최대 길이가 28, 그리고 평균 길이가 4로 Text에 비해 상대적으로 길이가 매우 짧음. 그래프로 봤을 때에도 대체적으로 10이하의 길이를 가짐.\n",
    "- 위의 결과를 바탕으로 텍스트의 길이와 서머리의 적절한 최대 길이를 임의로 정할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "entire-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 50\n",
    "summary_max_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afraid-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위의 임의로 정한 길이들이 얼마나 객관적인지 통계로 확인\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "favorite-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "#임의의 길이가 몇%의 샘플까지 포함하는지 볼 수 있음.\n",
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "anonymous-workplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 65818\n"
     ]
    }
   ],
   "source": [
    "#정해진 길이에 맞춰 자르는 것이 아닌 정해진 길이보다 길면 제외하는 방법으로 데이터 정제할 것.\n",
    "#잘라내면 텍스트 열의 경우 약 23%의 샘플들이 내용이 망가지기 때문(위의 결과)\n",
    "\n",
    "data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-intake",
   "metadata": {},
   "source": [
    "### - 시작 토큰과 종료 토큰 추가\n",
    "- 디코더는 시작 토큰을 입력받아 문장을 생성하기 시작하고, 종료 토큰을 예측한 순간에 문장 생성을 멈춤.\n",
    "- 이번 실습에서는 시작 토큰은 sostoken, 종료 토큰은 eostoken이라 임의로 명명하고 앞, 뒤로 추가함.\n",
    "- 디코더의 입력에 해당하면서 시작 토큰이 맨 앞에 있는 문장의 이름을 decoder_input, 디코더의 출력 또는 레이블에 해당되면서 종료 토큰이 맨 뒤에 붙는 문장의 이름을 decoder_target이라고 이름을 정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "wound-briefs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "valued-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코더의 입력, 디코더의 입력과 레이블을 다시 numpy 타입으로 저장.\n",
    "\n",
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eleven-mailman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57711 20402 41590 ... 56529  6007 24697]\n"
     ]
    }
   ],
   "source": [
    "#훈련 데이터와 테스트 데이터 분리\n",
    "### 직접 코딩을 통해 분리해 볼 것.\n",
    "#encoder_input과 크기, 형태가 같은 순서가 섞인 정수 시퀀스를 형성.\n",
    "\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "macro-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위의 정수 시퀀스를 이용해 데이터 샘플 순서를 정의하여 샘플을 섞음.\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "funky-scanning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 13163\n"
     ]
    }
   ],
   "source": [
    "#섞은 데이터를 8:2의 비율로 분리\n",
    "#전체 데이터의 크기에서 0.2를 곱해 테스트 데이터의 크기를 정의.\n",
    "\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "robust-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52655\n",
      "훈련 레이블의 개수 : 52655\n",
      "테스트 데이터의 개수 : 13163\n",
      "테스트 레이블의 개수 : 13163\n"
     ]
    }
   ],
   "source": [
    "#위에서 정의한 테스트 데이터의 개수를 이용해 전체 데이터를 양분\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-defense",
   "metadata": {},
   "source": [
    "## 단어 집합(vocabulary)만들기 및 정수 인코딩\n",
    "- 단어집합 : 각 단어에 고유한 정수를 맵핑\n",
    "- Keras의 토크나이저를 사용하면, 입력된 훈련 데이터로부터 단어 집합을 만들 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "homeless-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "incomplete-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 31888\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 23649\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8239\n",
      "단어 집합에서 희귀 단어의 비율: 74.16269443050678\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.3717768504802357\n"
     ]
    }
   ],
   "source": [
    "#빈도수가 낮은 단어들은 훈련 데이터에서 제외하고 진행.\n",
    "#등장 빈도수가 7회 미만인 단어들이 데이터에서 얼마나 차지하는지 확인\n",
    "\n",
    "#src_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장돼 있는데, 이를 통해서 통계적인 정보를 얻을 수 있음.\n",
    "\n",
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-cabin",
   "metadata": {},
   "source": [
    "- encoder_input_train에는 3만여 개의 단어가 있음.\n",
    "- 등장 빈도가 threshold 값인 7회 미만, 즉, 6회 이하인 단어들은 단어 집합에서 무려 70% 이상을 차지함.\n",
    "- 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 3.39%\n",
    "- 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거하는 것이 좋음.\n",
    "- 위에서 제외한 단어 집합 크기를 8천여개로 계산했는데 이와 비슷한 값으로 단어 집합 크기를 8천으로 제한할 것.\n",
    "- 토크나이저 정의 시 num_words의 값을 정하면 단어 집합의 크기를 제한할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "indian-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-blackjack",
   "metadata": {},
   "source": [
    "- texts_to_sequences()는 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩을 수행.\n",
    "- 8,000이 넘는 숫자들은 정수 인코딩 후에는 데이터에 존재하지 않게 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "preceding-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[779, 1442, 18, 245, 165, 1124, 629, 365, 897, 73, 165, 2814, 58, 73, 365, 165, 295, 3294], [139, 4270, 2597, 371, 2775, 5, 56, 54, 17, 43, 168, 907, 7, 7665, 1513, 230, 230, 108, 17, 2775, 91, 632, 768, 9, 144, 5, 2033, 230, 768, 237, 310, 516, 2, 839, 4633, 566, 50, 2049, 2775, 100, 68, 30], [12, 130, 724, 193, 608, 195, 202, 344, 233, 738, 608, 143, 238, 344, 285, 1345, 1117, 53, 2568, 283, 195, 117, 87]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "preceding-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary 데이터에 대해서도 동일하게 진행\n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "quality-publisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10480\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8097\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2383\n",
      "단어 집합에서 희귀 단어의 비율: 77.26145038167938\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.884529659303117\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "headed-nursing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1], [1, 5, 117, 1550, 61, 33, 398], [1, 6, 39, 4], [1, 3, 194], [1, 149]]\n",
      "target\n",
      "decoder  [[2], [5, 117, 1550, 61, 33, 398, 2], [6, 39, 4, 2], [3, 194, 2], [149, 2]]\n"
     ]
    }
   ],
   "source": [
    "#어림잡아 2,000을 단어 집합의 크기로 제한\n",
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-explosion",
   "metadata": {},
   "source": [
    "- 전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 이제 빈(empty) 샘플이 되었을 가능성이 있음.\n",
    "- 평균 길이가 짧았던 요약문은 더욱 심할 것.\n",
    "- decoder_input에는 sostoken 또는 decoder_target에는 eostoken이 추가된 상태이고, 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플 수와 동일하게 매우 높으므로 단어 집합 제한에도 삭제되지 않음. 그래서 이제 길이가 0이 된 요약문의 실제 길이는 1로 나올 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "comparative-camcorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1255\n",
      "삭제할 테스트 데이터의 개수 : 339\n",
      "훈련 데이터의 개수 : 51400\n",
      "훈련 레이블의 개수 : 51400\n",
      "테스트 데이터의 개수 : 12824\n",
      "테스트 레이블의 개수 : 12824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "#요약문에서 길이가 0이 된 샘플들의 인덱스를 받음.\n",
    "#훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 1인 경우의 인덱스를 \n",
    "#각각 drop_train과 drop_test에 라는 변수에 저장\n",
    "#이 샘플들은 모두 삭제할 것.\n",
    "\n",
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-reggae",
   "metadata": {},
   "source": [
    "## 패딩하기\n",
    "- 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰줌.\n",
    "- 최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 최대 길이로 길이를 맞춤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "coral-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-copyright",
   "metadata": {},
   "source": [
    "## 모델 설계\n",
    "- 함수형 API를 이용해 인코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "federal-discussion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "#임베딩 벡터의 차원은 128로 정의하고, hidden state의 크기를 256으로 정의\n",
    "embedding_dim = 128\n",
    "hidden_size = 256   # hidden state는 LSTM에서 얼만큼의 수용력(capacity)를 가질지를 정하는 파라미터\n",
    "                    #이 파라미터는 LSTM의 용량의 크기나, LSTM에서의 뉴런의 개수로 이해하면 됨.\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "#인코더의 LSTM은 총 3개의 층으로 구성해서 모델의 복잡도를 높임.\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "complimentary-namibia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "#디코더의 임베딩 층과 LSTM을 설계하는 것은 인코더와 거의 동일함.\n",
    "#LSTM 입력 정의 시 initial_state의 인자값으로 인코더의 hidden state와 ceel state의 값을 넣어줘야 함.\n",
    "\n",
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "southwest-moore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-moldova",
   "metadata": {},
   "source": [
    "- 디코더의 출력층에서는 Summary의 단어장인 tar_vocab의 수많은 선택지 중 하나의 단어를 선택하는 다중 클래스 분류 문제를 풀어야하기 때문에 Dense의 인자로 tar_vocab을 주고, 활성화 함수로 소프트맥스 함수를 사용함.   \n",
    "   \n",
    "- 지금까지 설계한 것은 인코더의 hidden state와 cell state를 디코더의 초기 state로 사용하는 가장 기본적인 seq2seq임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-lexington",
   "metadata": {},
   "source": [
    "## 어텐션 메커니즘\n",
    "- 디코더의 출력층을 설계를 살짝 바꿔서 성능을 높일 수 있는 방법.\n",
    "- 또 다른 새로운 신경망을 설계해야 한다는 뜻.\n",
    "- 이미 구현된 어텐션 함수를 가져와서 디코더의 출력층에 어떤 방식으로 결합하는지 배워볼 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "collectible-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#깃허브에 공개 되어 있는 어텐션 함수 다운\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "planned-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1024000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,276,432\n",
      "Trainable params: 4,276,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#디코더의 출력층 수정\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-potential",
   "metadata": {},
   "source": [
    "- 위의 코드는 인코더의 hidden state들과 디코더의 hidden state들을 어텐션 함수의 입력으로 사용하고, 어텐션 함수가 리턴한 값을 예측 시에 디코더의 hidden state와 함께 활용하는 형태로 작동함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-daughter",
   "metadata": {},
   "source": [
    "## 모델 훈련\n",
    "- '조기 종료'를 뜻하는 EarlyStopping은 특정 조건이 충족되면 훈련을 멈추는 역할."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "purple-optics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "201/201 [==============================] - 242s 1s/step - loss: 3.1216 - val_loss: 2.4067\n",
      "Epoch 2/50\n",
      "201/201 [==============================] - 228s 1s/step - loss: 2.4218 - val_loss: 2.2887\n",
      "Epoch 3/50\n",
      "201/201 [==============================] - 229s 1s/step - loss: 2.2728 - val_loss: 2.1510\n",
      "Epoch 4/50\n",
      "201/201 [==============================] - 229s 1s/step - loss: 2.1330 - val_loss: 2.0590\n",
      "Epoch 5/50\n",
      "201/201 [==============================] - 228s 1s/step - loss: 2.0314 - val_loss: 2.0041\n",
      "Epoch 6/50\n",
      "201/201 [==============================] - 227s 1s/step - loss: 1.9664 - val_loss: 1.9607\n",
      "Epoch 7/50\n",
      "201/201 [==============================] - 226s 1s/step - loss: 1.9071 - val_loss: 1.9331\n",
      "Epoch 8/50\n",
      "201/201 [==============================] - 225s 1s/step - loss: 1.8671 - val_loss: 1.9121\n",
      "Epoch 9/50\n",
      "201/201 [==============================] - 225s 1s/step - loss: 1.8156 - val_loss: 1.8915\n",
      "Epoch 10/50\n",
      "201/201 [==============================] - 224s 1s/step - loss: 1.7791 - val_loss: 1.8775\n",
      "Epoch 11/50\n",
      "201/201 [==============================] - 224s 1s/step - loss: 1.7473 - val_loss: 1.8674\n",
      "Epoch 12/50\n",
      "201/201 [==============================] - 226s 1s/step - loss: 1.7103 - val_loss: 1.8613\n",
      "Epoch 13/50\n",
      "201/201 [==============================] - 225s 1s/step - loss: 1.6903 - val_loss: 1.8536\n",
      "Epoch 14/50\n",
      "201/201 [==============================] - 225s 1s/step - loss: 1.6539 - val_loss: 1.8491\n",
      "Epoch 15/50\n",
      "201/201 [==============================] - 225s 1s/step - loss: 1.6266 - val_loss: 1.8473\n",
      "Epoch 16/50\n",
      "201/201 [==============================] - 225s 1s/step - loss: 1.6067 - val_loss: 1.8440\n",
      "Epoch 17/50\n",
      "201/201 [==============================] - 226s 1s/step - loss: 1.5769 - val_loss: 1.8452\n",
      "Epoch 18/50\n",
      "201/201 [==============================] - 225s 1s/step - loss: 1.5572 - val_loss: 1.8435\n",
      "Epoch 19/50\n",
      "201/201 [==============================] - 225s 1s/step - loss: 1.5409 - val_loss: 1.8476\n",
      "Epoch 20/50\n",
      "201/201 [==============================] - 226s 1s/step - loss: 1.5194 - val_loss: 1.8443\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "#val_loss(검증 데이터의 손실)을 관찰하다가, \n",
    "#검증 데이터의 손실이 줄어들지 않고 증가하는 현상이 2회(patience=2) 관측되면 학습을 멈추도록 설정\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "raising-service",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#훈련 데이터의 손실과 검증 데이터의 손실이 줄어드는 과정 시각화\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-assembly",
   "metadata": {},
   "source": [
    "## 인퍼런스 모델 구현하기\n",
    "- 테스트 단계에서 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "closed-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 사전 3개\n",
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-burning",
   "metadata": {},
   "source": [
    "- seq2seq는 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행해야 함.\n",
    "- 정답 문장이 없는 인퍼런스 단계에서는 만들어야 할 문장의 길이만큼 디코더가 반복 구조로 동작해야 하기 때문에 부득이하게 인퍼런스를 위한 모델 설계를 별도로 해주어야 함.\n",
    "- 인코더 모델과 디코더 모델을 분리해서 설계."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bulgarian-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sharing-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 어텐션 메커니즘 사용하는 출력층 설계\n",
    "\n",
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "green-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 인퍼런스 단계에서 단어 시퀀스를 완성하는 함수\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-tractor",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "realistic-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-location",
   "metadata": {},
   "source": [
    "- Text의 정수 시퀀스에서는 패딩을 위해 사용되는 숫자 0을 제외하고 Summary의 정수 시퀀스에서는 숫자 0, 시작 토큰의 인덱스, 종료 토큰의 인덱스를 출력에서 제외하도록 만들어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "electrical-resolution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : great nutritional bar good breakfast contain lots fiber great snack anytime keep hand great weight loss well \n",
      "실제 요약 : great bars \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : first love hot sweet teas one right benefits read drinking sage tea good cleansing type diabetes immune booster helps rid throat tea pretty good aside benefits purchasing \n",
      "실제 요약 : love this tea \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : nuts would satisfy palate lover nuts product high quality price nuts rich fresh flavour \n",
      "실제 요약 : delicious \n",
      "예측 요약 :  good popcorn\n",
      "\n",
      "\n",
      "원문 : love sweet crispy delicious healthy dried fried problem addictive cannot stop eating \n",
      "실제 요약 : love these \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : best flavor amazing crisp right time tried brands cut buying lot wish walmart still carried \n",
      "실제 요약 : best ever \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : looking something substitute pasta great asian dishes like noodle soup stir fry etc love side benefit good want wheat pasta substitute try rice pasta cook \n",
      "실제 요약 : great noodle \n",
      "예측 요약 :  great pasta\n",
      "\n",
      "\n",
      "원문 : new velveeta oz chicken broccoli cheesy skillet kit high protein quick tasty meal five cup servings cook chicken broccoli mixture one pound chicken cups water minutes added fresh broccoli onion cheese additional taste volume criticism seasoning blend partially hydrogenated soybean oil \n",
      "실제 요약 : quick and tasty meal \n",
      "예측 요약 :  not as good as other\n",
      "\n",
      "\n",
      "원문 : bought jimmies could find needed orange color local stores pound jimmies lot went expected probably half bag thoroughly coat recipe chocolate truffles taste well taste disappointment surprise short jimmies two thirds length wilton brands cupboard need special color makes hesitate get debbie lee \n",
      "실제 요약 : exactly what needed \n",
      "예측 요약 :  not for me\n",
      "\n",
      "\n",
      "원문 : bought vinegar along others father christmas wish could shipped together went separately good vinegar though \n",
      "실제 요약 : beautiful vinegar \n",
      "예측 요약 :  great gift\n",
      "\n",
      "\n",
      "원문 : coffee perfect like strong coffee shop coffee mild yet bold flavor actually make two cups cup helps save lots money works perfectly want one cup morning second cup lighter still good flavor \n",
      "실제 요약 : delicious \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : drink mountain dew recently black favorite soda tried change logo add words fruit punch know bad logo change someone unfortunatly yet find substitute times cannot find get \n",
      "실제 요약 : my favorite soda but so hard to find \n",
      "예측 요약 :  great tasting\n",
      "\n",
      "\n",
      "원문 : tasty fill would think great hunger little meal everything tried fiber one nothing short delicious awesome \n",
      "실제 요약 : so good \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : delicious flavorful tea must cinnamon lovers also pleasant sweetness even us use sweeteners enjoy wonderful hot drink especially like cold drink \n",
      "실제 요약 : if you like cinnamon \n",
      "예측 요약 :  great tea\n",
      "\n",
      "\n",
      "원문 : yuck taste good quite tough better buying bags rinds already \n",
      "실제 요약 : yuck \n",
      "예측 요약 :  not that great\n",
      "\n",
      "\n",
      "원문 : good coffee pods high quality traditional \n",
      "실제 요약 : good coffee \n",
      "예측 요약 :  good coffee\n",
      "\n",
      "\n",
      "원문 : powder best superfood product purchased stronger antioxidant anything found \n",
      "실제 요약 : this stuff is awesome to say the least \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : tryed dessert several years ago amazing thing ever sadly never saw shelves happy get amazon even though pkgs \n",
      "실제 요약 : thought it was gone \n",
      "예측 요약 :  the best\n",
      "\n",
      "\n",
      "원문 : bought beans saeco go much better starbuck using since first got found feeding burnt beans like starbuck espresso cause problems machine coffee far better taste enough kick boot makes nice cappuccino espresso good crema cheaper lavazza beans amazon hopefully raise price \n",
      "실제 요약 : super coffee \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : another great whole bean coffee bean direct drink lot coffee like quality quantity go coffee bean direct tried one like brew hearty aromatic bitter great coffee buying five pound bag way go think coffee direct best deal overall \n",
      "실제 요약 : another great bean \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : makes good cup coffee strong bitter flavorful stands creamer still taste coffee \n",
      "실제 요약 : very good cup of coffee \n",
      "예측 요약 :  good coffee\n",
      "\n",
      "\n",
      "원문 : decided buy one grocery store buying case glad got two bites disgusting brownie tossing rest bar garbage going waste treat something tastes mixture cardboard chemicals whether chemicals bars tastes like counts starving death bars get thin nuff said rather eat fruit \n",
      "실제 요약 : no zero stars available \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : bought make green tea cake cupcakes opened package smaller bag thought going guess little goes long way great price \n",
      "실제 요약 : smaller bad than thought \n",
      "예측 요약 :  tea\n",
      "\n",
      "\n",
      "원문 : incredible grateful find product husband drinks oz juice twice daily eliminated chronic gout symptoms free fast shipping packaged well \n",
      "실제 요약 : husband \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : bold rich love coffee fast wake morning pick mid day name perfectly suited delightful brew must store airtight container opening plastic bags though coffee lose \n",
      "실제 요약 : coffee \n",
      "예측 요약 :  great coffee\n",
      "\n",
      "\n",
      "원문 : buying kind bars year must say every bar wonderful use snack breakfast find keeps satisfied completely almond walnut macadamia bars perfectly delicious whole nuts every bite heaven earth \n",
      "실제 요약 : perfectly delicious \n",
      "예측 요약 :  great snack\n",
      "\n",
      "\n",
      "원문 : impression got buying dinos received one good deal last time order rip \n",
      "실제 요약 : beware you only get \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : fancy grade good gets disappointed supports proud vermont tradition \n",
      "실제 요약 : vermont maple syrup at its best \n",
      "예측 요약 :  great licorice\n",
      "\n",
      "\n",
      "원문 : received product pleased quality assortment savings local store prices order \n",
      "실제 요약 : great taste and variety \n",
      "예측 요약 :  great service\n",
      "\n",
      "\n",
      "원문 : great product would recommend anyone friends ate like packages one night better movie popcorn \n",
      "실제 요약 : popcorn review \n",
      "예측 요약 :  great popcorn\n",
      "\n",
      "\n",
      "원문 : dog small loves loves treats treats toss air run around could get idea taste really thought point would give give bottle passing friend trying train dog pull leash realize training tool dog seem think like taste \n",
      "실제 요약 : my dog did not get it \n",
      "예측 요약 :  great treats\n",
      "\n",
      "\n",
      "원문 : salad dressing turns lettuce tomato shrimp gourmet meal even better open lump crab meat substitute shrimp make summer delight \n",
      "실제 요약 : makes into meal \n",
      "예측 요약 :  not much sugar\n",
      "\n",
      "\n",
      "원문 : wow product simply delicious nice texture almost like shredded beef spices flavor right combination smokey salty sweet problem trying stop eating know packed protein low fat calories carb content seems pretty high one strip watch consumption although replaced meal two primal strips lost pounds week exceptional meat alternative snack \n",
      "실제 요약 : seriously delish \n",
      "예측 요약 :  great flavor\n",
      "\n",
      "\n",
      "원문 : pros free super saving shipping small juicy taste flavors healthy cons sometimes seller vs order like several packs \n",
      "실제 요약 : energy that works \n",
      "예측 요약 :  great taste\n",
      "\n",
      "\n",
      "원문 : look like good idea wife thought would give try pretty easy use thing like mess make machine even top secure leak machine keep cleaning filter basket gets sticky would advise use something better like cup \n",
      "실제 요약 : could be better \n",
      "예측 요약 :  good value\n",
      "\n",
      "\n",
      "원문 : disappointed time actually tried several nong shim noodles like noodle got one really impressed texture noodles kimchi noodle say typical nong shim noodle totally different texture lack nothing better brands \n",
      "실제 요약 : not typical noodle \n",
      "예측 요약 :  not bad\n",
      "\n",
      "\n",
      "원문 : dressing awesome tasty full chemical ingredients dressings wish would discovered earlier \n",
      "실제 요약 : very tasty \n",
      "예측 요약 :  great soup\n",
      "\n",
      "\n",
      "원문 : son several allergies looking anything could use would pass bread school lunches hoping would good enough surprised chebe actually good liked second batch tried make dough came sticky called chebe customer service helped us figure problem fast friendly even replaced batch perfect sandwiches lunch jelly breakfast \n",
      "실제 요약 : very good \n",
      "예측 요약 :  great product\n",
      "\n",
      "\n",
      "원문 : brownies good flax seed texture real turn minimize added little whole wheat flour splenda cocoa big improvement guests never complained \n",
      "실제 요약 : good healthy dessert but needs work \n",
      "예측 요약 :  delicious\n",
      "\n",
      "\n",
      "원문 : using zuke mini snack month siberian husky fantastic starting obedience classes required find something really went guess trainer way making training easier natural product amazing chewy savory right picky dog pup except pup prefers beef flavor thankfully becoming well dog \n",
      "실제 요약 : quality dog snack \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 요약 :  perfect for training\n",
      "\n",
      "\n",
      "원문 : enjoy flavor brand noodle nice light easy flavor want little flavor splash soy sauce easy store prepare eat love always cupboard got get \n",
      "실제 요약 : soooooo \n",
      "예측 요약 :  very good\n",
      "\n",
      "\n",
      "원문 : recently tried pocket coffee friend brought back italy one best chocolate ever searched brought product amazon extremely disappointed get italy instant maxwell coffee middle hershey chocolate compare real thing \n",
      "실제 요약 : do not waste your money \n",
      "예측 요약 :  best coffee ever\n",
      "\n",
      "\n",
      "원문 : first try unsalted un roasted nuts back seemed fresh enough suppose taste bland continue risk life deliciously roasted salted stuff \n",
      "실제 요약 : yuck \n",
      "예측 요약 :  not that good\n",
      "\n",
      "\n",
      "원문 : ordered night plants arrived usps priority mail noon saturday plants packed box foam peanuts top pots sealed tape around plant stems potting soil still damp plant inches tall arrived green healthy \n",
      "실제 요약 : quick shipping plants arrived healthy \n",
      "예측 요약 :  great bonsai tree\n",
      "\n",
      "\n",
      "원문 : bought dog takes medicine needs eat foods low sodium mix pedigree healthy heart low sodium chicken usually pick eat later put anything else think take getting use \n",
      "실제 요약 : alright \n",
      "예측 요약 :  great for dogs with allergies\n",
      "\n",
      "\n",
      "원문 : real tast treat try sour foods taste sugary sweet apples strawberries oranges taste like delicious fruit ever makes fun party activity guests wonderful vinegar candy like limes \n",
      "실제 요약 : it is \n",
      "예측 요약 :  love these\n",
      "\n",
      "\n",
      "원문 : family loves stuff sooo good used buy time local grocery store moved another state one sells resort buying line simply cannot live without yr old son favorite side usually serve salmon roasted chicken \n",
      "실제 요약 : we love it \n",
      "예측 요약 :  love this product\n",
      "\n",
      "\n",
      "원문 : great product great price dog many food allergies sensitivities great find healthy treat loves also good price \n",
      "실제 요약 : great product \n",
      "예측 요약 :  great for training\n",
      "\n",
      "\n",
      "원문 : citric acid great cleaning hard water deposits coffee pots etc smells like lemons works quickly use ounces per quart several food uses also \n",
      "실제 요약 : acid for \n",
      "예측 요약 :  works well\n",
      "\n",
      "\n",
      "원문 : like granny smith kind light dusting sour powder ate whole box approx days please bring item back \n",
      "실제 요약 : so good \n",
      "예측 요약 :  sour\n",
      "\n",
      "\n",
      "원문 : product flavorful need much recipe really strong organic \n",
      "실제 요약 : organic \n",
      "예측 요약 :  good stuff\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-convert",
   "metadata": {},
   "source": [
    "- 성능을 개선하기 위해서는 seq2seq와 어텐션의 자체의 조합을 좀 더 좋게 수정하는 방법도 있고, 빔 서치(beam search), 사전 훈련된 워드 임베딩(pre-trained word embedding), 또는 인코더 - 디코더 자체의 구조를 새로이 변경한 하는 트랜스포머(Transformer)와 같은 여러 개선 방안들이 존재함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-crime",
   "metadata": {},
   "source": [
    "## 추출적 요약\n",
    "- 패키지 Summa에서는 추출적 요약을 위한 모듈인 summarize를 제공하고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "chinese-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 다운\n",
    "\n",
    "import requests\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "#매트릭스 시놉시스\n",
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "obvious-oxford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The screen is filled with green, cascading code which gives way to the title, The Matrix.\r\n",
      "\r\n",
      "A phone rings and text appears on the screen: \"Call trans opt: received. 2-19-98 13:24:18 REC: Log>\" As a conversation takes place between Trinity (Carrie-Anne Moss) and Cypher (Joe Pantoliano), two free humans, a table of random green numbers are being scanned and individual numbers selected, creating a series of digits not unlike an ordinary phone number, as if a code is being deciphered or a call is being traced.\r\n",
      "\r\n",
      "Trinity discusses some unknown person. Cypher taunts Trinity, suggesting she enjoys watching him. Trinity counters that \"Morpheus (Laurence Fishburne) says he may be 'the One',\" just as the sound of a number being selected alerts Trinity that someone may be tracing their call. She ends the call.\r\n",
      "\r\n",
      "Armed policemen move down a darkened, decrepit hallway in the Heart O' the City Hotel, their flashlight beam bouncing just ahead of them. They come to room 303, kick down the door and find a woman dressed in black, facing away from them. It's Trinity. She brings her hands up from the laptop she's working on at their command.\r\n",
      "\r\n",
      "Outside the hotel a car drives up and three agents appear in neatly pressed black suits. They are Agent Smith (Hugo Weaving), Agent Brown (Paul Goddard), and Agent Jones (Robert Taylor). Agent Smith and the presiding police lieutenant argue. Agent Smith admonishes the policeman that they were given specific orders to contact the agents first, for their\n"
     ]
    }
   ],
   "source": [
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-wichita",
   "metadata": {},
   "source": [
    "### - summarize \n",
    "- text (str) : 요약할 테스트.\n",
    "- ratio (float, optional) – 요약문에서 원본에서 선택되는 문장 비율. 0~1 사이값\n",
    "- words (int or None, optional) – 출력에 포함할 단어 수.\n",
    "- 만약, ratio와 함께 두 파라미터가 모두 제공되는 경우 ratio는 무시한다.\n",
    "- split (bool, optional) – True면 문장 list / False는 조인(join)된 문자열을 반환\n",
    "\n",
    "- Summa의 summarize는 문장 토큰화를 별도로 하지 않더라도 내부적으로 문장 토큰화를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "union-lease",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "#원문의 0.005%만을 출력\n",
    "print(summarize(text, ratio=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bizarre-macro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "['Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.', 'Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.']\n"
     ]
    }
   ],
   "source": [
    "#리스트로 출력 결과 받음\n",
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "coupled-tulsa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Trinity takes Neo to Morpheus.\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "#단어 50개만 선택하도록\n",
    "print('Summary:')\n",
    "print(summarize(text, words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-disclosure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
