{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suspected-israeli",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 문장: <start> 나는 밥을 먹었다 \n",
      "Target 문장:  나는 밥을 먹었다 <end>\n"
     ]
    }
   ],
   "source": [
    "sentence = \" 나는 밥을 먹었다 \"\n",
    "\n",
    "#문장 시작을 알리는 <start> \n",
    "source_sentence = \"<start>\" + sentence\n",
    "#문장 마지막을 알리는 <end>\n",
    "target_sentence = sentence + \"<end>\"\n",
    "\n",
    "print(\"Source 문장:\", source_sentence)\n",
    "print(\"Target 문장:\", target_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mighty-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 파일을 읽기모드로 열고\n",
    "# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "# 앞에서부터 10라인만 화면에 출력해 볼까요?\n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premier-booking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-physiology",
   "metadata": {},
   "source": [
    "### 문장만 출력하고 싶을 때 \n",
    "- 화자가 표기된 문장(0,3,6), 공백인 문장(2,5,9)를 제외시켜야 함\n",
    "- 화자가 표기된 문장은 문장 끝이 : 로 끝남. \n",
    "- 공백인 문장은 길이가 0.\n",
    "- 문장 끝이 : 로 끝나거나 문장 길이가 0이면 제외시키는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-hampton",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "- 텍스트 생성 모델에도 단어 사전을 만들게 됨.\n",
    "- 문장을 일정한 기준으로 쪼개야 함.\n",
    "- 이 과정을 토큰화라고 함.\n",
    "- 가장 간단한 방법은 띄어쓰기를 기준으로 나누기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-great",
   "metadata": {},
   "source": [
    "# 데이터 전처리 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dutch-discrimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장에서\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지움\n",
    "#     2. 특수문자 양쪽에 공백을 넣기\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿈\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿈\n",
    "#     5. 다시 양쪽 공백을 지움\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가\n",
    "\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있음!\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-marina",
   "metadata": {},
   "source": [
    "### 소스 문장 \n",
    "- 자연어 처리 분야에서 모델의 입력이 되는 문장\n",
    "\n",
    "### 타겟 문장\n",
    "- 정답 역할을 하게 될 모델의 출력 문장   \n",
    "\n",
    "### 정제 데이터 구축\n",
    "- 정제 함수를 통해 만든 데이터셋에서 토큰화를 진행 후 끝 단어 < end >를 없애면 소스 문장이 됨.\n",
    "- 첫 단어 < start >를 없애면 타켓 문장이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "severe-sussex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 준비\n",
    "\n",
    "#정제된 문장을 넣을 빈 리스트\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 원하지 않는 문장은 건너뜀\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    #정제 후 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-tiffany",
   "metadata": {},
   "source": [
    "## 벡터화\n",
    "- tf.keras.preprocessing.text.Tokenizer 패키지는 정제된 데이터를 토큰화하고, 단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며, 데이터를 숫자로 변환까지 해줌.\n",
    "\n",
    "### 텐서(tensor)\n",
    "- 숫자로 변환된 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "north-landscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f980ca1ef10>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요(빈칸)\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "earned-christopher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "agricultural-farmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "#텐서 데이터는 모두 정수, tokenizer에 구축 된 단어 사전의 인덱스임.\n",
    "\n",
    "#구축 된 단어 사전 출력\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "latin-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "#생성된 텐서를 소스와 타켓으로 분리하여 모델이 학습할 수 있게 함\n",
    "#텐서 출력부에서 행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩(padding)을 채워 넣은 것\n",
    "#0은 사전에는 없지만 패딩문자 <pad>가 될 것\n",
    "\n",
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "conventional-tennis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터셋 객체 형성\n",
    "#이전에는 model.fit으로 모델에 데이터셋 제공했음\n",
    "#텐서플로우는 텐서로 생성된 데이터를 이용해 tf.data.Dataset객체를 생성하는 방법을 흔히 사용\n",
    "#tf.data.Dataset객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의 기능을 제공\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-blogger",
   "metadata": {},
   "source": [
    "# 데이터셋 생성 과정 정리 (데이터 전처리)\n",
    "- 정규표현식을 이용한 corpus 생성\n",
    "- tf.keras.prprocessing.text.Tokenizer를 이용해 courpus를 텐서로 변환\n",
    "- tf.data.Dataset.from_tensor_slices()를 이용해 courpus 텐서를 tf.data.Dataset 객체로 변환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-architecture",
   "metadata": {},
   "source": [
    "# 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "broken-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense\n",
    "#Embedding 레이어 ; 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔 줌\n",
    "#워드벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용됨\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "#embedding_size :  워드 벡터의 차원수, 단어가 추상적으로 표현되는 크기\n",
    "#여기서는 256으로 정함. 데이터가 너무 많으면 혼란을 야기하기 때문\n",
    "embedding_size = 256\n",
    "\n",
    "#LSTM hiddenstate의 차원수 = hidden_size : 모델에 얼마나 많은 일꾼을 둘 것인지\n",
    "#일꾼들은 같은 데이터를 보고 각자의 생각을 가지게 됨.\n",
    "#충분한 데이터가 주어지면 올바른 결정을 내림.\n",
    "#여기서는 1024로 정함. 역시 데이터가 너무 많으면 올바른 결정 못하기 때문\n",
    "hidden_size = 1024\n",
    "\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cooperative-belief",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[ 1.71337640e-04, -2.74114398e-04,  7.86561600e-07, ...,\n",
       "          3.66533413e-06,  4.08343942e-04, -1.62882294e-04],\n",
       "        [ 9.39842430e-05, -8.04304655e-05, -4.62705357e-05, ...,\n",
       "          4.35437367e-04,  6.65245985e-04, -3.00455460e-04],\n",
       "        [ 1.54409063e-04, -1.89665327e-04,  2.82444496e-04, ...,\n",
       "          9.30823211e-04,  7.51157582e-04, -9.86828090e-05],\n",
       "        ...,\n",
       "        [ 5.11249877e-04, -3.89733585e-04, -3.62372794e-03, ...,\n",
       "         -5.02393988e-04,  7.86205637e-04,  5.86953771e-04],\n",
       "        [ 5.94834681e-04, -4.79911483e-04, -3.75270797e-03, ...,\n",
       "         -4.09254280e-04,  7.44833029e-04,  5.19899768e-04],\n",
       "        [ 6.55893935e-04, -5.70120930e-04, -3.85342096e-03, ...,\n",
       "         -3.22057167e-04,  6.99475990e-04,  4.47636558e-04]],\n",
       "\n",
       "       [[ 1.71337640e-04, -2.74114398e-04,  7.86561600e-07, ...,\n",
       "          3.66533413e-06,  4.08343942e-04, -1.62882294e-04],\n",
       "        [ 2.43550967e-04, -4.49213141e-04, -1.84203804e-04, ...,\n",
       "         -1.72343818e-04,  4.37334005e-04, -5.37559914e-04],\n",
       "        [ 1.69466744e-04, -5.18325542e-04, -4.26991464e-04, ...,\n",
       "         -3.82312806e-04,  4.43513767e-04, -5.65746566e-04],\n",
       "        ...,\n",
       "        [ 3.73989460e-04,  1.38095056e-04, -2.07668776e-03, ...,\n",
       "          1.25036779e-04,  5.97245526e-04,  8.52692814e-04],\n",
       "        [ 4.93575353e-04, -2.18627210e-05, -2.46158801e-03, ...,\n",
       "          6.69675774e-06,  6.49461115e-04,  7.83774420e-04],\n",
       "        [ 6.06651069e-04, -1.62979879e-04, -2.78347568e-03, ...,\n",
       "         -6.81097081e-05,  6.85808191e-04,  7.17371993e-04]],\n",
       "\n",
       "       [[ 1.71337640e-04, -2.74114398e-04,  7.86561600e-07, ...,\n",
       "          3.66533413e-06,  4.08343942e-04, -1.62882294e-04],\n",
       "        [-6.58659847e-05, -1.83363634e-04, -3.19491490e-04, ...,\n",
       "          4.62863914e-04,  7.68152764e-04,  8.30455538e-05],\n",
       "        [-4.40053496e-04, -9.38635203e-05, -5.22286049e-04, ...,\n",
       "          3.07599810e-04,  7.25493475e-04,  6.61279133e-04],\n",
       "        ...,\n",
       "        [ 3.17939179e-04, -5.85213187e-04, -3.13998200e-03, ...,\n",
       "         -1.17347343e-03,  4.44490783e-04,  3.26737209e-04],\n",
       "        [ 4.34135087e-04, -6.21198793e-04, -3.41483345e-03, ...,\n",
       "         -1.06832618e-03,  4.79968730e-04,  3.11365700e-04],\n",
       "        [ 5.38298336e-04, -6.56693475e-04, -3.62637779e-03, ...,\n",
       "         -9.50768532e-04,  5.01191535e-04,  2.91725009e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.71337640e-04, -2.74114398e-04,  7.86561600e-07, ...,\n",
       "          3.66533413e-06,  4.08343942e-04, -1.62882294e-04],\n",
       "        [ 4.06926323e-04, -5.62148925e-04, -5.81179665e-05, ...,\n",
       "          2.28736142e-04,  5.79904823e-04,  1.79441377e-05],\n",
       "        [ 5.05683711e-04, -5.66322182e-04,  2.81688728e-04, ...,\n",
       "          4.39679250e-04,  6.47729088e-04,  1.77368434e-04],\n",
       "        ...,\n",
       "        [-5.92143042e-04, -2.69713200e-04, -1.79990823e-03, ...,\n",
       "         -2.46683485e-04, -2.90564076e-05,  1.85013458e-03],\n",
       "        [-4.56260168e-04, -4.94542997e-04, -2.25103297e-03, ...,\n",
       "         -4.09899687e-04,  1.01045407e-04,  1.72845961e-03],\n",
       "        [-2.92957324e-04, -6.81739533e-04, -2.63466476e-03, ...,\n",
       "         -5.14743093e-04,  2.25164084e-04,  1.60178437e-03]],\n",
       "\n",
       "       [[ 1.71337640e-04, -2.74114398e-04,  7.86561600e-07, ...,\n",
       "          3.66533413e-06,  4.08343942e-04, -1.62882294e-04],\n",
       "        [ 5.94652083e-04, -7.53544911e-04,  3.08731192e-04, ...,\n",
       "         -1.25259656e-04,  7.38598173e-04, -5.01287686e-05],\n",
       "        [ 9.08161048e-04, -1.05254480e-03,  4.16070630e-04, ...,\n",
       "         -2.85474322e-04,  6.23545086e-04,  2.81464436e-05],\n",
       "        ...,\n",
       "        [-2.04735643e-05, -6.83966035e-04, -2.86698877e-03, ...,\n",
       "         -4.33762383e-04,  1.03211415e-03,  2.29334168e-04],\n",
       "        [ 1.40686243e-04, -6.66412176e-04, -3.12754954e-03, ...,\n",
       "         -3.47710040e-04,  1.02464599e-03,  2.27309458e-04],\n",
       "        [ 2.82523164e-04, -6.60413876e-04, -3.34276212e-03, ...,\n",
       "         -2.68036732e-04,  1.00088981e-03,  2.15082822e-04]],\n",
       "\n",
       "       [[ 1.71337640e-04, -2.74114398e-04,  7.86561600e-07, ...,\n",
       "          3.66533413e-06,  4.08343942e-04, -1.62882294e-04],\n",
       "        [ 3.47540976e-04, -5.14080690e-04,  2.56338099e-04, ...,\n",
       "         -1.00027166e-04,  8.11841048e-04, -1.50469685e-04],\n",
       "        [ 3.92479851e-04, -2.67602009e-04,  2.08148151e-04, ...,\n",
       "         -4.95931192e-04,  1.12411985e-03, -2.02032985e-04],\n",
       "        ...,\n",
       "        [-6.84254395e-04,  8.41572939e-04, -1.73517992e-03, ...,\n",
       "         -5.77268656e-04,  1.28622240e-04,  4.67362639e-04],\n",
       "        [-5.42125723e-04,  5.95192949e-04, -2.22249166e-03, ...,\n",
       "         -6.31369126e-04,  1.78638395e-04,  5.26596501e-04],\n",
       "        [-3.65068699e-04,  3.68553709e-04, -2.63462262e-03, ...,\n",
       "         -6.51041511e-04,  2.37555549e-04,  5.62047120e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "\n",
    "#모델에 데이터를 아주 조금 태워보는 것\n",
    "#모델이 아직 제대로 build되지 않았기 때문\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-potato",
   "metadata": {},
   "source": [
    "- 출력된 shape를 보면 (256,20,7001)임\n",
    "- 7001은 Dense 레이어의 출력 차원 수\n",
    "- 7001개 단어 중 어느 단어의 확률이 가장 높은지 모델링 해야 하기 대문\n",
    "- 256은 배치 사이즈, dataset.take(1)을 통해 1개의 배치, 256개의 문장 데이터를 가져온 것\n",
    "- 20 : tf.keras.layers.LSTM(hidden_size, return_sequences=True)로 호출한 LSTM 레이어에서 return_sequences=True 라고 지정한 부분 때문에, 본인에게 입력된 시퀀스 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미가 되므로 20이 나오게 된 것.\n",
    "- 만약 return_sequences=False 였으면 LSTM 레이어는 1개의 벡터만 출력할 것.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-myanmar",
   "metadata": {},
   "source": [
    "- 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모르는데 20은 어떻게?\n",
    "  - 데이터를 입력받으면서 알게 된 것. 데이터셋의  max_len이 20으로 맞춰져 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "atlantic-chicago",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-renaissance",
   "metadata": {},
   "source": [
    "- 모델이 입력 시퀀스의 길이를 모르기 대문에 Output Shape를 특정할 수 없음.\n",
    "- 모델 파라미터 사이즈는 측정 됨. 22million 정도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-union",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bearing-force",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 40s 399ms/step - loss: 4.3968\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 37s 397ms/step - loss: 2.8134\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 37s 399ms/step - loss: 2.7207\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 2.6196\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 38s 403ms/step - loss: 2.5395\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 38s 405ms/step - loss: 2.4927\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 2.4393\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 38s 404ms/step - loss: 2.3951\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 38s 404ms/step - loss: 2.3348\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 2.2824\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 38s 403ms/step - loss: 2.2487\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 2.1933\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 38s 403ms/step - loss: 2.1479\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 2.1079\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 2.0572\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 2.0225\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.9741\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 1.9304\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 37s 403ms/step - loss: 1.8826\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 1.8378\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 37s 402ms/step - loss: 1.7975\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 1.7538\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 1.7106\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 1.6683\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 1.6242\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 1.5774\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 1.5334\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 37s 401ms/step - loss: 1.4913\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 37s 399ms/step - loss: 1.4466\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 37s 399ms/step - loss: 1.3998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f980c5d6cd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-stress",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "animal-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    \n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-relevance",
   "metadata": {},
   "source": [
    "- while문\n",
    "  - 텍스트를 실제로 생성해야 하는 시점\n",
    "  - 타겟문장과 소스 문장이 없음\n",
    "  - 텍스트 생성 태스크를 위해 테스트 데이터셋을 따로 생성해야 함\n",
    "  - generate_text() 함수에서 init_sentence를 인자로 받아 텐서로 만들고 있음. 디폴트로는 < start > 단어 하나만 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "numeric-storm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a changeling and two of my <unk> <end> '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "confident-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she is not fourteen . <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "nearby-syndication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i ll tell you what you shall arraign my lord , <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-great",
   "metadata": {},
   "source": [
    "# 프로젝트 : 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-burlington",
   "metadata": {},
   "source": [
    "## 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vocal-circle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "#여러개 텍스트 파일 읽어서 raw_corpus에 저장\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-jacket",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerous-canyon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was petrified\n",
      "I grew strong\n",
      "I will survive\n",
      "Yeah\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0 or len(sentence) > 15 : continue\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "    \n",
    "\n",
    "    if idx > 20: break  \n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developed-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ongoing-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "#정제된 문장을 넣을 빈 리스트\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 원하지 않는 문장은 건너뜀\n",
    "    if len(sentence) == 0 or len(sentence) > 15 : continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    #정제 후 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "harmful-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    4   98 ...    0    0    0]\n",
      " [   2    4 1076 ...    0    0    0]\n",
      " [   2    4  111 ...    0    0    0]\n",
      " ...\n",
      " [   2    4   21 ...    0    0    0]\n",
      " [   2    4   21 ...    0    0    0]\n",
      " [   2    4   21 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fd6483b5450>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    #corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    #준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    #입력 데이터의 시퀀스 길이를 일정하게 맞춤\n",
    "    #만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춤\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 15)  \n",
    "    #문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "yellow-intake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    4   98 1336    3    0    0    0    0    0    0    0    0    0]\n",
      "[   4   98 1336    3    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "#tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "\n",
    "#마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "src_input = tensor[:, :-1]  \n",
    "\n",
    "#tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cardiac-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14120, 15)\n",
      "(14120, 14)\n",
      "(14120, 14)\n"
     ]
    }
   ],
   "source": [
    "print(tensor.shape)\n",
    "print(src_input.shape)\n",
    "print(tgt_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-science",
   "metadata": {},
   "source": [
    "## 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "protecting-sitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (11296, 15)\n",
      "Target Train: (11296, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val =  train_test_split(tensor, \n",
    "                                                    tensor, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=7)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-vampire",
   "metadata": {},
   "source": [
    "## 모델 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "searching-break",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 256)         3072000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, None, 1024)        5246976   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, None, 1024)        8392704   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 12000)       12300000  \n",
      "=================================================================\n",
      "Total params: 29,011,680\n",
      "Trainable params: 29,011,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lib_size = tokenizer.num_words\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(lib_size, embedding_size))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(lib_size))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "burning-canadian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "353/353 [==============================] - 30s 79ms/step - loss: 2.2471\n",
      "Epoch 2/10\n",
      "353/353 [==============================] - 28s 79ms/step - loss: 0.7757\n",
      "Epoch 3/10\n",
      "353/353 [==============================] - 28s 79ms/step - loss: 0.5726\n",
      "Epoch 4/10\n",
      "353/353 [==============================] - 28s 80ms/step - loss: 0.4405\n",
      "Epoch 5/10\n",
      "353/353 [==============================] - 28s 80ms/step - loss: 0.3387\n",
      "Epoch 6/10\n",
      "353/353 [==============================] - 28s 80ms/step - loss: 0.2658\n",
      "Epoch 7/10\n",
      "353/353 [==============================] - 28s 80ms/step - loss: 0.2092\n",
      "Epoch 8/10\n",
      "353/353 [==============================] - 28s 80ms/step - loss: 0.1640\n",
      "Epoch 9/10\n",
      "353/353 [==============================] - 28s 80ms/step - loss: 0.1328\n",
      "Epoch 10/10\n",
      "353/353 [==============================] - 28s 80ms/step - loss: 0.1075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd6642bf5d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-hollywood",
   "metadata": {},
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "postal-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 함수\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "complex-lincoln",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love love love love love love love love love love love love love love love love love love '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adjacent-virginia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i am am am am am am am am am am am am am am am am am am '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i am\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "internal-donna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you are\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-ceremony",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
