{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "robust-selection",
   "metadata": {},
   "source": [
    "# Single Stage Object Detection\n",
    "## One-Stage Detection\n",
    "### YOLO and SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-retail",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-garden",
   "metadata": {},
   "source": [
    "## YOLO : You Only Look Once\n",
    "- RCNN 계열의 가정 : \"물체가 존재할 것 같은 곳을 backbone network로 표현할 수 있다.\" → region proposal network\n",
    "- YOLO의 예측값은 S x S x (B * 5 + C) 크기의 텐서로 출력됨. 이를 flatten하면 다 저 텐서를 계산한 값이 나옴.\n",
    "- YOLO의 목표는 grid에 포함되는 물체를 잘 잡아내는 것.\n",
    "- 학습 목표가 제대로 이루어졌는지를 확인하려면 객체 인식 모델의 성능 평가 도구인 IoU(Intersection over Union)를 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-assurance",
   "metadata": {},
   "source": [
    "## YOLO v1\n",
    "- YOLO v1의 가정 : \"이미지 내의 작은 영역을 나누면 그곳에 물체가 있을 수 있다.\" → grid 내에 물체가 존재한다.\n",
    "- YOLO v1에서 grid는 고정되고, 각 grid 안에 물체가 있을 확률이 중요하게 됨.\n",
    "- YOLO는 이미지를 S x S grid로 나누고, 각 grid cell은 bounding box(bbox)와 각 box의 confidence score를 예측함.\n",
    "- confidence score는 bbox가 사물(object)을 포함하는지를 모델이 얼마나 확신하는지(confident), 그리고 box가 그 사물을 얼마나 정확히 예측하는지를 보여주는 점수임.\n",
    "- 각 bbox는 x, y, w, h, confidence, 총 5개의 예측을 하는데, (x, y) 좌표는 bbox의 중심 좌표이고 w와 h는 너비와 높이.\n",
    "- 각 grid cell은 C개의 조건부 class 확률도 예측하는데 이 확률은 grid cell이 사물을 포함할 때를 조건으로 하는 확률로 bounding box의 개수와 상관 없이 각 grid cell의 확률만 예측함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-finnish",
   "metadata": {},
   "source": [
    "### NMS(Non-Maximum Suppression)\n",
    "- 습이 잘된 경우는 해당 grid들이 모두 비슷한 크기로 bbox를 잡는데 동일한 물체를 잡는 bbox가 많아진다는 문제가 있음.\n",
    "- 비-최대 억제라고도 불리는 NMS 기법을 이용함. 이는 object detector가 예측한 bounding box 중 정확한 bounding box를 선택하는 기법임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-poetry",
   "metadata": {},
   "source": [
    "## YOLO v1 단점\n",
    "- 각각 grid cell이 하나의 클래스만 예측 가능하므로 작은 object에 대해 예측이 어려움. \n",
    "- bbox의 형태가 training data를 통해 학습되었기 때문에 bbox 분산이 너무 넓어 새로운 형태의 bbox 예측이 잘 안됨.\n",
    "- 모델 구조상 backbone만 거친 feature map을 대상으로 bbox 정보를 예측하기 때문에 localization이 부정확함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-amendment",
   "metadata": {},
   "source": [
    "## YOLO v2\n",
    "- YOLO v1보다 정확도를 더 올리고 속도를 향상시키며 더 많은 범위의 class를 예측하기 위해 발전됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-purse",
   "metadata": {},
   "source": [
    "## YOLO v3\n",
    "- [관련 논문](https://taeu.github.io/paper/deeplearning-paper-yolov3/)\n",
    "- YOLO v2 보다 더 발전됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-junior",
   "metadata": {},
   "source": [
    "## SSD : Single Shot MultiBox Detector\n",
    "- SSD는 YOLO v1에서 grid를 사용해서 생기는 단점을 해결할 수 있는 Image Pyramid, Pre-defined Anchor Box 같은 테크닉을 제안함.\n",
    "- [관련 논문](https://arxiv.org/pdf/1512.02325.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-computer",
   "metadata": {},
   "source": [
    "### Image Pyramid\n",
    "- ImageNet으로 사전학습된 VGG16을 사용하며 VGG에서 pooling layer를 거친 block은 하나의 image feature로 사용 가능함.\n",
    "- SSD는 38x38, 19x19, 10x10, 5x5, 3x3 등의 다양한 크기의 feature map을 사용함.\n",
    "- 각 feature map은 YOLO의 관점에서 보면 원본 이미지에서 grid 크기를 다르게 하는 효과가 있었으며 5 x 5 크기의 feature map에서 grid가 너무 커서 small object를 못찾는 문제를 38 x 38 크기의 feature map에서 찾을 수 있게 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-blues",
   "metadata": {},
   "source": [
    "### SSD의 Workflow\n",
    "- bounding box가 존재한다면, 그 bounding box만의 x, y, w, h 특성이 존재하기 때문에 pre-defined된 box의 x, y, w, h를 refinement하는 layer를 추가하는 것이 좋으며 anchor box를 SSD에서는 Default box라고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-click",
   "metadata": {},
   "source": [
    "## FCOS\n",
    "- [관련 논문](https://arxiv.org/pdf/1904.01355.pdf)\n",
    "- 위의 논문은 기존의 anchor box기반에서 벗어나 pixelwise로 예측하는 FCOS를 제안한 내용임.\n",
    "-  Anchor box를 사용하면서 생기는 부작용을 해결하면서 좋은 성능도 보임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-intellectual",
   "metadata": {},
   "source": [
    "## Face Detection 관련 논문\n",
    "- [S3FD](https://seongkyun.github.io/papers/2019/03/21/S3FD/)\n",
    "- [DSFD](https://arxiv.org/pdf/1810.10220.pdf)\n",
    "- [RetinaFace](https://arxiv.org/pdf/1905.00641.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-forwarding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
